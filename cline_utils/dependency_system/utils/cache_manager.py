"""
Cache management module with dynamic, TTL-based caching for dependency tracking system.
Supports on-demand cache creation, automatic expiration, and granular invalidation.
"""

import functools
import gzip
import json
import logging
import os
import pickle
import re
import threading
import time
import uuid
from dataclasses import dataclass, field
from enum import Enum
from typing import Any, Callable, Dict, List, Optional, Tuple, TypeVar, cast

logger = logging.getLogger(__name__)

F = TypeVar("F", bound=Callable[..., Any])

# Configuration
CACHE_DIR = os.path.join(os.path.dirname(os.path.abspath(__file__)), "cache")
DEFAULT_MAX_SIZE = 5000  # Default max items per cache
DEFAULT_TTL = 300  # 10 minutes in seconds
CACHE_SIZES = {
    "embeddings_generation": 150,  # Smaller for heavy data
    "key_generation": 5000,  # Larger for key maps
    "reranking": 1000,  # Reranking results
    "default": DEFAULT_MAX_SIZE,
}

# Advanced cache configuration
ENABLE_COMPRESSION = True
COMPRESSION_THRESHOLD = 1024  # Only compress items larger than 1KB
COMPRESSION_MIN_SAVINGS = 0.1  # 10% minimum savings


class EvictionPolicy(Enum):
    """Cache eviction policies."""

    LRU = "lru"  # Least Recently Used
    LFU = "lfu"  # Least Frequently Used
    ADAPTIVE = "adaptive"  # Hybrid LRU/LFU
    FIFO = "fifo"  # First In, First Out
    RANDOM = "random"  # Random eviction


@dataclass
class CacheMetrics:
    """Enhanced cache performance metrics."""

    hits: int = 0
    misses: int = 0
    evictions: int = 0
    compression_saves: int = 0
    total_size_bytes: int = 0
    access_count: Dict[str, int] = field(default_factory=dict)
    last_access: Dict[str, float] = field(default_factory=dict)

    @property
    def hit_rate(self) -> float:
        """Calculate hit rate percentage."""
        total = self.hits + self.misses
        return (self.hits / total * 100) if total > 0 else 0.0


class Cache:
    """Enhanced cache instance with LRU/LFU eviction, compression, TTL, and dependency tracking."""

    def __init__(
        self,
        name: str,
        ttl: int = DEFAULT_TTL,
        max_size: int = DEFAULT_MAX_SIZE,
        eviction_policy: EvictionPolicy = EvictionPolicy.LRU,
        enable_compression: bool = ENABLE_COMPRESSION,
    ):
        self.name = name
        self.data: Dict[str, Tuple[Any, float, Optional[float]]] = (
            {}
        )  # (value, access_time, expiry_time)
        self.dependencies: Dict[str, List[str]] = {}  # key -> dependent keys
        self.reverse_deps: Dict[str, List[str]] = {}  # key -> keys that depend on it
        self.creation_time = time.time()
        self.default_ttl = ttl
        self.max_size = CACHE_SIZES.get(name, max_size)
        self.eviction_policy = eviction_policy
        self.enable_compression = enable_compression

        # Enhanced features
        self.metrics = CacheMetrics()  # This will call __post_init__ automatically
        self._lock = threading.RLock()  # Thread safety
        self.compression_threshold = COMPRESSION_THRESHOLD

        # Adaptive parameters
        self.lru_weight = 0.6 if eviction_policy == EvictionPolicy.ADAPTIVE else 1.0
        self.min_accesses = 3

        logger.debug(
            f"Cache '{name}' initialized: policy={eviction_policy.value}, "
            f"max_size={self.max_size}, compression={enable_compression}"
        )

    def get(self, key: str) -> Any:
        with self._lock:
            if key not in self.data:
                self.metrics.misses += 1
                logger.debug(f"Cache '{self.name}': Miss for key '{key}'")
                return None

            # Get value and metadata
            value, access_time, expiry = self.data[key]

            # Check if expired
            if expiry and time.time() > expiry:
                self._remove_key(key)
                self.metrics.misses += 1
                logger.debug(f"Cache '{self.name}': Miss (expired) for key '{key}'")
                return None

            # Update access information
            current_time = time.time()
            self.data[key] = (value, current_time, expiry)  # Update access time
            self.metrics.last_access[key] = current_time
            self.metrics.access_count[key] = self.metrics.access_count.get(key, 0) + 1
            self.metrics.hits += 1
            logger.debug(f"Cache '{self.name}': Hit for key '{key}'")

            # Decompress if necessary
            if isinstance(value, bytes) and self.enable_compression:
                value = self._decompress_value(value)

            return value

    def _compress_value(self, value: Any) -> bytes:
        """Compress value for storage."""
        if isinstance(value, str):
            return gzip.compress(value.encode("utf-8"))
        else:
            return gzip.compress(pickle.dumps(value))

    def _decompress_value(self, value: bytes) -> Any:
        """Decompress value from storage."""
        try:
            decompressed = gzip.decompress(value)
            # Try to decode as string first
            try:
                return decompressed.decode("utf-8")
            except UnicodeDecodeError:
                # Fall back to unpickling
                return pickle.loads(decompressed)
        except Exception as e:
            logger.warning(f"Failed to decompress value: {e}")
            # Return raw bytes if decompression fails
            return value

    def _estimate_size(self, value: Any) -> int:
        """Estimate size of a value in bytes."""
        try:
            if isinstance(value, str):
                return len(value.encode("utf-8"))
            elif isinstance(value, bytes):
                return len(value)
            else:
                return len(pickle.dumps(value))
        except:
            return len(str(value))

    def _should_compress(self, key: str, value: Any) -> bool:
        """Determine if value should be compressed."""
        if not self.enable_compression:
            return False

        size = self._estimate_size(value)

        # Only compress if above threshold
        if size < self.compression_threshold:
            return False

        # Estimate compression ratio
        try:
            compressed = gzip.compress(pickle.dumps(value))
            ratio = len(compressed) / size
            return ratio < (1 - COMPRESSION_MIN_SAVINGS)
        except:
            return False

    def set(
        self,
        key: str,
        value: Any,
        dependencies: Optional[List[str]] = None,
        ttl: Optional[int] = None,
    ) -> None:
        with self._lock:
            # Compress if beneficial
            if self._should_compress(key, value):
                value = self._compress_value(value)
                self.metrics.compression_saves += 1

            # Calculate size for metrics
            size_estimate = self._estimate_size(value)

            # Evict if necessary
            if len(self.data) >= self.max_size:
                self._evict_items()

            # Set with new metadata
            expiry = (
                time.time() + (ttl if ttl is not None else self.default_ttl)
                if ttl != 0
                else None
            )
            self.data[key] = (value, time.time(), expiry)
            self.metrics.total_size_bytes += size_estimate

            # Track dependencies
            if dependencies:
                for dep in dependencies:
                    if dep not in self.dependencies:
                        self.dependencies[dep] = []
                    self.dependencies[dep].append(key)
                    if key not in self.reverse_deps:
                        self.reverse_deps[key] = []
                    self.reverse_deps[key].append(dep)

    def _evict_items(self) -> None:
        """Enhanced eviction with multiple policies."""
        items_to_evict = len(self.data) - self.max_size

        if items_to_evict <= 0:
            return

        if self.eviction_policy == EvictionPolicy.LFU:
            # Evict least frequently used items
            freq_items = sorted(
                self.data.keys(), key=lambda k: self.metrics.access_count.get(k, 0)
            )
            keys_to_evict = freq_items[:items_to_evict]
        elif self.eviction_policy == EvictionPolicy.FIFO:
            # Evict first in, first out
            keys_to_evict = list(self.data.keys())[:items_to_evict]
        elif self.eviction_policy == EvictionPolicy.RANDOM:
            import random

            keys = list(self.data.keys())
            keys_to_evict = random.sample(keys, min(items_to_evict, len(keys)))
        else:  # LRU or ADAPTIVE
            # For LRU/ADAPTIVE, use access time sorting
            time_sorted = sorted(
                self.data.keys(), key=lambda k: self.data[k][1]  # access_time
            )
            keys_to_evict = time_sorted[:items_to_evict]

        # Remove evicted items
        for key in keys_to_evict:
            self._remove_key(key)
            self.metrics.evictions += 1

    def get_stats(self) -> Dict[str, Any]:
        """Get enhanced cache statistics."""
        with self._lock:
            total_items = len(self.data)
            total_size = self.metrics.total_size_bytes

            return {
                "name": self.name,
                "total_items": total_items,
                "max_size": self.max_size,
                "utilization": (
                    (total_items / self.max_size * 100) if self.max_size > 0 else 0
                ),
                "total_size_bytes": total_size,
                "total_size_mb": total_size / (1024 * 1024),
                "hit_rate": self.metrics.hit_rate,
                "hits": self.metrics.hits,
                "misses": self.metrics.misses,
                "evictions": self.metrics.evictions,
                "compression_saves": self.metrics.compression_saves,
                "eviction_policy": self.eviction_policy.value,
                "compression_enabled": self.enable_compression,
            }

    def _evict_lru(self) -> None:
        if not self.data:
            return
        try:
            lru_key = min(self.data, key=lambda k: self.data[k][1])
            self._remove_key(lru_key)
        except ValueError:
            pass
        except RuntimeError:
            logger.warning(
                f"Cache '{self.name}': RuntimeError during LRU eviction. Cache may be highly contended."
            )

    def _remove_key(self, key: str) -> None:
        try:
            if key in self.data:
                del self.data[key]

            # Clean up reverse dependencies: remove 'key' from the dependency lists of its dependents
            if key in self.reverse_deps:
                for dependent_key in list(
                    self.reverse_deps.get(key, [])
                ):  # Iterate a copy
                    if (
                        dependent_key in self.dependencies
                    ):  # Check if dependent_key still in self.dependencies
                        try:
                            self.dependencies[dependent_key].remove(key)
                            if not self.dependencies[dependent_key]:
                                del self.dependencies[dependent_key]
                        except (ValueError, KeyError):
                            pass  # Item already removed or dep dict changed
                try:
                    del self.reverse_deps[key]  # Remove key from reverse_deps itself
                except KeyError:
                    pass

        except KeyError:
            pass

    def cleanup_expired(self) -> None:
        """Remove all expired entries."""
        current_time = time.time()
        try:
            items_to_check = list(self.data.items())
        except RuntimeError:
            logger.warning(
                f"Cache '{self.name}': RuntimeError getting items for cleanup, likely high contention. Skipping this cleanup cycle."
            )
            return

        expired_keys_to_remove = [
            k
            for k, (_, _, expiry) in items_to_check
            if expiry and current_time > expiry
        ]

        if expired_keys_to_remove:
            keys_actually_removed_count = 0
            for key_to_remove in expired_keys_to_remove:
                if key_to_remove in self.data:
                    _val, _acc_time, exp_check_final = self.data[key_to_remove]
                    if exp_check_final and current_time > exp_check_final:
                        self._remove_key(key_to_remove)
                        keys_actually_removed_count += 1
            if keys_actually_removed_count > 0:
                logger.debug(
                    f"Cache '{self.name}': Cleaned up {keys_actually_removed_count} expired entries."
                )

    def is_expired(self) -> bool:
        return (time.time() - self.creation_time) > self.default_ttl and not self.data

    def invalidate(self, key_pattern: str) -> None:
        """Invalidate entries matching a key pattern (supports regex). Also invalidates dependent entries."""
        with self._lock:  # <<< SUGGESTION: Add lock here
            compiled_pattern = re.compile(key_pattern)
            # Iterate over a copy of keys for safety during removal
            keys_to_remove_initial = [
                k for k in list(self.data.keys()) if compiled_pattern.match(k)
            ]

            processed_for_invalidation = set()
            queue_to_invalidate = list(keys_to_remove_initial)

            while queue_to_invalidate:
                key_to_invalidate = queue_to_invalidate.pop(0)
                if key_to_invalidate in processed_for_invalidation:
                    continue

                self._remove_key(
                    key_to_invalidate
                )  # Handles removal from self.data and basic reverse_deps cleanup
                processed_for_invalidation.add(key_to_invalidate)

                if key_to_invalidate in self.dependencies:
                    dependent_keys_list = list(
                        self.dependencies.get(key_to_invalidate, [])
                    )
                    for dep_key in dependent_keys_list:
                        if dep_key not in processed_for_invalidation:
                            queue_to_invalidate.append(dep_key)
                    try:
                        del self.dependencies[key_to_invalidate]
                    except KeyError:
                        pass

            if processed_for_invalidation:
                logger.debug(
                    f"Cache '{self.name}': Invalidated {len(processed_for_invalidation)} entries matching pattern '{key_pattern}'."
                )

    def stats(self) -> Dict[str, int]:
        return {
            "hits": self.metrics.hits,
            "misses": self.metrics.misses,
            "size": len(self.data),
            "evictions": self.metrics.evictions,
            "compression_saves": self.metrics.compression_saves,
        }


class CacheManager:
    """Manages multiple caches with persistence and cleanup."""

    def __init__(self, persist: bool = False):
        self.caches: Dict[str, Cache] = {}
        self.persist = persist
        if persist:
            os.makedirs(CACHE_DIR, exist_ok=True)
            self._load_persistent_caches()

    def get_cache(self, cache_name: str, ttl: int = DEFAULT_TTL) -> Cache:
        """Retrieve or create a cache by name."""
        if cache_name not in self.caches or self.caches[cache_name].is_expired():
            self.caches[cache_name] = Cache(cache_name, ttl)
            logger.debug(f"Spun up new cache: {cache_name} with TTL {ttl}s")
        return self.caches[cache_name]

    def cleanup(self) -> None:
        """Remove expired caches."""
        expired = [
            name for name, cache in list(self.caches.items()) if cache.is_expired()
        ]
        for name in expired:
            if self.persist:
                self._save_cache(name)
            if name in self.caches:
                del self.caches[name]
                logger.debug(f"Spun down expired cache: {name}")
        for cache in list(self.caches.values()):
            cache.cleanup_expired()

    def clear_all(self) -> None:
        if self.persist:
            for name in list(self.caches.keys()):
                self._save_cache(name)
        self.caches.clear()
        logger.info("All caches cleared.")

    def _save_cache(self, cache_name: str) -> None:
        if cache_name in self.caches:
            cache_file = os.path.join(CACHE_DIR, f"{cache_name}.json")
            # Use UUID to ensure uniqueness across threads/processes
            temp_file = f"{cache_file}.{uuid.uuid4()}.tmp"
            try:
                with open(temp_file, "w", encoding="utf-8") as f:
                    # Ensure there's data to write
                    if not self.caches[cache_name].data:
                        # If cache is empty, don't write an empty file, just ensure old one is gone
                        if os.path.exists(cache_file):
                            try:
                                os.remove(cache_file)
                            except OSError:
                                pass
                        return

                    current_cache_data_items = list(
                        self.caches[cache_name].data.items()
                    )

                    def _json_safe(obj: Any) -> Any:
                        """
                        Convert cache values into JSON-serializable form.
                        - Pass through primitives and plain containers.
                        - For bytes and other non-serializable objects, store a safe representation.
                        """
                        # Fast path: already JSON-native
                        if isinstance(obj, (str, int, float, bool)) or obj is None:
                            return obj
                        if isinstance(obj, dict):
                            return {str(k): _json_safe(v) for k, v in obj.items()}
                        if isinstance(obj, (list, tuple, set)):
                            return [_json_safe(v) for v in obj]
                        if isinstance(obj, (bytes, bytearray)):
                            # Avoid breaking on compressed/serialized cache entries
                            # Represent as tagged hex string instead of raw bytes
                            return {
                                "__type__": "bytes",
                                "encoding": "hex",
                                "data": obj.hex(),
                            }
                        # Fallback: string representation
                        return repr(obj)

                    data = {
                        "data": {
                            k: _json_safe(v[0])
                            for k, v in current_cache_data_items
                            if v[2] is None or v[2] > time.time()
                        },
                        "dependencies": dict(self.caches[cache_name].dependencies),
                    }
                    json.dump(data, f)

                # Atomic rename with retries for Windows file locking
                max_retries = 3
                for i in range(max_retries):
                    try:
                        os.replace(temp_file, cache_file)
                        break
                    except OSError as e:
                        if i == max_retries - 1:
                            raise  # Re-raise on last attempt
                        logger.warning(
                            f"Retry {i+1}/{max_retries} saving cache {cache_name}: {e}"
                        )
                        time.sleep(0.1)  # Short wait before retry

            except Exception as e:
                logger.error(f"Failed to save cache {cache_name}: {e}")
            finally:
                # Clean up temp file if it still exists
                if os.path.exists(temp_file):
                    try:
                        os.remove(temp_file)
                    except OSError:
                        pass

    def _json_revive(self, obj: Any) -> Any:
        """Recursively revive objects from their JSON-safe representation."""
        if isinstance(obj, dict):
            if obj.get("__type__") == "bytes" and obj.get("encoding") == "hex":
                try:
                    return bytes.fromhex(obj.get("data", ""))
                except (ValueError, TypeError):
                    return obj  # Return as-is if hex is invalid
            return {k: self._json_revive(v) for k, v in obj.items()}
        if isinstance(obj, list):
            return [self._json_revive(v) for v in obj]
        return obj

    def _load_persistent_caches(self) -> None:
        if not os.path.exists(CACHE_DIR):
            return
        for cache_file in os.listdir(CACHE_DIR):
            if cache_file.endswith(".json"):
                cache_name = cache_file[:-5]
                cache_path = os.path.join(CACHE_DIR, cache_file)
                try:
                    # Check if file is empty to prevent JSONDecodeError
                    if os.path.getsize(cache_path) == 0:
                        logger.warning(
                            f"Cache file {cache_file} is empty. Deleting it."
                        )
                        os.remove(cache_path)
                        continue

                    with open(cache_path, "r", encoding="utf-8") as f:
                        data = json.load(f)
                        cache = Cache(cache_name)
                        for key, value in data.get("data", {}).items():
                            revived_value = self._json_revive(value)
                            cache.set(key, revived_value, ttl=0)
                        cache.dependencies = data.get("dependencies", {})
                        self.caches[cache_name] = cache
                    logger.debug(f"Loaded persistent cache: {cache_name}")
                except json.JSONDecodeError as e:
                    logger.error(
                        f"Failed to decode JSON from {cache_file}: {e}. Deleting corrupt cache file."
                    )
                    try:
                        os.remove(cache_path)
                    except OSError as oe:
                        logger.error(
                            f"Error removing corrupt cache file {cache_path}: {oe}"
                        )
                except Exception as e:
                    logger.error(
                        f"Failed to load cache {cache_name} from {cache_file}: {e}"
                    )


cache_manager = CacheManager(persist=False)


def get_tracker_cache_key(tracker_path: str, tracker_type: str) -> str:
    from .path_utils import normalize_path

    return f"tracker:{normalize_path(tracker_path)}:{tracker_type}"


def clear_all_caches() -> None:
    """Clear all caches in the manager."""
    cache_manager.clear_all()


def invalidate_dependent_entries(cache_name: str, key_pattern: str) -> None:
    """Invalidate cache entries matching a key pattern in a specific cache."""
    cache = cache_manager.get_cache(cache_name)
    cache.invalidate(key_pattern)


def file_modified(file_path: str, project_root: str, cache_type: str = "all") -> None:
    """Invalidate caches when a file is modified."""
    from .path_utils import normalize_path

    norm_path = normalize_path(file_path)
    # Use raw f-string for regex pattern
    key_pattern_to_invalidate = (
        rf".*(?::|\||^){re.escape(norm_path)}(?:\||$).*"  # FIXED
    )

    caches_to_scan = (
        [
            cache_manager.get_cache(cache_name)
            for cache_name in list(cache_manager.caches.keys())
        ]
        if cache_type == "all"
        else [cache_manager.get_cache(cache_type)]
    )

    for cache_instance in caches_to_scan:
        if cache_instance:
            cache_instance.invalidate(key_pattern_to_invalidate)
    logger.debug(
        f"Invalidated entries matching path '{norm_path}' (pattern '{key_pattern_to_invalidate}') in cache(s) type '{cache_type}'."
    )


def tracker_modified(
    tracker_path: str, tracker_type: str, project_root: str, cache_type: str = "all"
) -> None:
    """Invalidate caches when a tracker is modified."""
    from .path_utils import normalize_path

    norm_path = normalize_path(tracker_path)
    key_pattern_for_tracker: str

    if cache_type == "all":
        # Use raw f-string for regex pattern
        generic_path_pattern = rf".*(?::|\||^){re.escape(norm_path)}(?:\||$).*"  # FIXED
        specific_tracker_data_pattern = (
            rf"^tracker_data_structured:{re.escape(norm_path)}:.*"  # FIXED
        )

        cache_data_struct = cache_manager.get_cache("tracker_data_structured")
        if cache_data_struct:
            cache_data_struct.invalidate(specific_tracker_data_pattern)
            logger.debug(
                f"Invalidated '{specific_tracker_data_pattern}' in 'tracker_data_structured'."
            )

        for cache_name_iter in list(cache_manager.caches.keys()):
            if cache_name_iter != "tracker_data_structured":
                cache_instance_iter = cache_manager.get_cache(cache_name_iter)
                if cache_instance_iter:
                    cache_instance_iter.invalidate(generic_path_pattern)
        logger.debug(
            f"Additionally scanned other caches for generic pattern '{generic_path_pattern}'."
        )

    else:
        if cache_type == "tracker_data_structured":
            key_pattern_for_tracker = (
                rf"^tracker_data_structured:{re.escape(norm_path)}:.*"  # FIXED
            )
        else:
            key_pattern_for_tracker = (
                rf".*(?::|\||^){re.escape(norm_path)}(?:\||$).*"  # FIXED
            )

        cache_instance_tracker = cache_manager.get_cache(cache_type)
        if cache_instance_tracker:
            cache_instance_tracker.invalidate(key_pattern_for_tracker)
        logger.debug(
            f"Invalidated entries for tracker '{norm_path}' (pattern '{key_pattern_for_tracker}') in cache type '{cache_type}'."
        )


def cached(
    cache_name: str,
    key_func: Optional[Callable[..., str]] = None,
    ttl: Optional[int] = DEFAULT_TTL,
):
    """Decorator for caching with dynamic dependencies and TTL."""

    def decorator(func: F) -> F:
        @functools.wraps(func)
        def wrapper(*args: Any, **kwargs: Any) -> Any:
            # Prefer provided key_func; otherwise use a robust default that avoids __func__ checks
            if key_func is not None:
                key = key_func(*args, **kwargs)
            else:
                # Default key: function name + stringified args/kwargs (excluding "self"/"cls" heuristically)
                arg_list = list(args)
                if (
                    arg_list
                    and (arg_list[0].__class__.__name__ != "str")
                    and hasattr(arg_list[0], "__class__")
                ):
                    # Heuristic: if first arg looks like an instance/class, omit it from cache key
                    # This avoids fragile __func__ introspection that triggers Pylance errors.
                    arg_list_for_key = arg_list[1:]
                else:
                    arg_list_for_key = arg_list
                d_key_parts = [str(a) for a in arg_list_for_key] + [
                    f"{k}={v}" for k, v in sorted(kwargs.items())
                ]
                key = f"{func.__name__}::{'|'.join(d_key_parts)}"

            cache_ttl_to_use = ttl if ttl is not None else DEFAULT_TTL
            cache = cache_manager.get_cache(cache_name, cache_ttl_to_use)

            cached_val = cache.get(key)
            if cached_val is not None:
                return cached_val

            result = func(*args, **kwargs)

            # Extract dependencies if function returns (value, [deps]) convention
            dependencies_list_from_result: List[str] = []
            value_to_cache: Any = result
            if (
                isinstance(result, tuple)
                and len(result) == 2
                and isinstance(result[1], list)
            ):
                value_to_cache, dependencies_list_from_result = result
            elif func.__name__ in [
                "load_embedding",
                "load_metadata",
                "analyze_file",
                "analyze_project",
                "get_file_type",
            ]:
                # Try to infer a file path dependency from first non-self/cls argument
                actual_first_arg_for_dep: Optional[Any] = args[0] if args else None
                if args:
                    # If it appears to be a method, path arg is likely the second arg
                    if hasattr(args[0], "__class__"):
                        actual_first_arg_for_dep = args[1] if len(args) > 1 else None
                if isinstance(actual_first_arg_for_dep, str) and os.path.exists(
                    actual_first_arg_for_dep
                ):
                    from .path_utils import normalize_path

                    dependencies_list_from_result.append(
                        f"file:{normalize_path(actual_first_arg_for_dep)}"
                    )

            cache.set(
                key, value_to_cache, dependencies_list_from_result, ttl=cache_ttl_to_use
            )
            cache_manager.cleanup()
            return value_to_cache

        return cast(F, wrapper)

    return decorator


def check_file_modified(file_path: str) -> bool:
    """Check if a file has been modified, updating metadata cache."""
    from .path_utils import get_project_root, normalize_path

    norm_path = normalize_path(file_path)
    cache_key = f"timestamp:{norm_path}"
    cache = cache_manager.get_cache("metadata")
    current_project_root = get_project_root()

    if not os.path.exists(file_path):
        if cache.get(cache_key) is not None:
            cache.invalidate(cache_key)
            file_modified(norm_path, current_project_root)
            return True
        return False

    current_timestamp = os.path.getmtime(file_path)
    cached_timestamp_val = cache.get(cache_key)

    if cached_timestamp_val is None or current_timestamp > cached_timestamp_val:
        cache.set(cache_key, current_timestamp, ttl=None)
        file_modified(norm_path, current_project_root)
        return True
    return False


def get_file_type_cached(file_path: str) -> str:
    """Cached version of get_file_type."""
    from .path_utils import get_file_type

    return get_file_type(file_path)


def get_cache_stats(cache_name: str) -> Dict[str, int]:
    """Get hit/miss stats for a cache."""
    cache = cache_manager.get_cache(cache_name)
    return cache.stats()
