import pytest
import os
import time
import shutil
import json
from pathlib import Path
import numpy as np

# Assuming cache_manager and relevant functions are importable
# Adjust imports based on actual file structure and function locations
from cline_utils.dependency_system.utils.cache_manager import CacheManager, cached, get_cache_stats, clear_all_caches
from cline_utils.dependency_system.utils import path_utils
from cline_utils.dependency_system.utils import config_manager
from cline_utils.dependency_system.analysis import embedding_manager # <<< _is_valid_file is here
from cline_utils.dependency_system.core import dependency_grid
from cline_utils.dependency_system.core.dependency_grid import compress # Import compress
from cline_utils.dependency_system.io import tracker_io
from cline_utils.dependency_system.analysis import dependency_analyzer
# Import for integration tests
from cline_utils.dependency_system.analysis.project_analyzer import analyze_project
# Import command handler for MT-01
from cline_utils.dependency_system.dependency_processor import handle_clear_caches # Use the specific handler function

# Helper function to touch a file (update mtime)
def touch(filepath):
    """Updates the modification time of a file."""
    try:
        Path(filepath).touch()
    except OSError as e:
        print(f"Error touching file {filepath}: {e}")

# --- Fixtures ---

@pytest.fixture(scope="function") # Removed autouse=True
def clear_cache_fixture(): # Renamed slightly for clarity
    """Ensures a clean cache state before each test function runs."""
    # Use the actual clear cache mechanism provided by cache_manager
    # Assuming a global CacheManager instance or a callable clear function
    clear_all_caches() # Adjust if the clearing mechanism is different
    yield # Test runs here
    clear_all_caches() # Optional: clear after test too

@pytest.fixture(scope="session")
def temp_test_dir(tmp_path_factory):
    """Create a temporary directory unique to the test session."""
    return tmp_path_factory.mktemp("cache_tests")

@pytest.fixture(scope="function")
def test_project(temp_test_dir):
    """Sets up a minimal temporary project structure for testing."""
    project_dir = temp_test_dir / "test_project"
    project_dir.mkdir(exist_ok=True) # Allow directory to exist
    (project_dir / ".clinerules").touch()
    (project_dir / ".clinerules.config.json").write_text(json.dumps({
        "paths": {"doc_dir": "docs", "memory_dir": "cline_docs"},
        "exclusions": {"dirs": [".git", "venv"], "files": ["*.log"]},
        "thresholds": {"code_similarity": 0.7}
    }))
    # Ensure all necessary directories are created with exist_ok=True
    (project_dir / "src").mkdir(exist_ok=True)
    (project_dir / "src" / "module_a.py").write_text("import os\nprint('hello')")
    (project_dir / "src" / "module_b.py").write_text("print('world')")
    (project_dir / "docs").mkdir(exist_ok=True)
    (project_dir / "docs" / "readme.md").write_text("# Test Readme")
    (project_dir / "cline_docs").mkdir(exist_ok=True)
    
    # Add another potential source directory and file for IS-03 test
    (project_dir / "lib").mkdir(exist_ok=True)
    (project_dir / "lib" / "helper.py").write_text("def helper_func(): return 1")

    # Create dummy cache dir and embedding files for testing FC-03 etc.
    cache_dir = project_dir / "cache"
    cache_dir.mkdir(exist_ok=True)
    embedding_dir = cache_dir / "embeddings"
    embedding_dir.mkdir(exist_ok=True)
    # Mock embedding data
    np.save(embedding_dir / "1A1.npy", np.array([0.1, 0.2]))
    np.save(embedding_dir / "1B2.npy", np.array([0.3, 0.4]))
    np.save(embedding_dir / "1C3.npy", np.array([0.5, 0.6]))

    # Mock the config to point to this cache dir for embeddings
    # This might require mocking ConfigManager().get_cache_dir() or similar
    # For simplicity here, we might need to adjust embedding_manager calls
    # Or assume calculate_similarity takes the embedding dir path

    # Store original CWD and change to project dir for tests needing it
    original_cwd = os.getcwd()
    os.chdir(project_dir)
    yield project_dir # Provide the path to the test functions
    os.chdir(original_cwd) # Restore original CWD
    # Return the project directory path for use in tests
    return project_dir
# --- Functional Tests (FC-01 to FC-07) ---

# Test Case FC-01: path_utils.get_project_root Cache
def test_fc01_get_project_root_cache(test_project, clear_cache_fixture, caplog): # Added fixture arg
    clear_all_caches() # Manual clear at start
    """Verify get_project_root cache hits and potential invalidation."""
    # Assume cache name is 'get_project_root', adjust if needed
    cache_name = 'get_project_root'
    project_root_path = test_project

    # 1. Initial call
    print(f"Calling get_project_root for the first time from: {os.getcwd()}")
    root1 = path_utils.get_project_root()
    # Normalize both paths before comparison
    assert path_utils.normalize_path(root1) == path_utils.normalize_path(str(project_root_path))
    # Don't assert initial miss count, focus on subsequent behavior
    stats1 = get_cache_stats(cache_name)

    # 2. Second call - should be a cache hit
    print("Calling get_project_root for the second time...")
    root2 = path_utils.get_project_root()
    assert path_utils.normalize_path(root2) == path_utils.normalize_path(root1) # Compare normalized
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Expect at least one hit on the second call
    # assert stats2['hits'] > stats1.get('hits', 0) # This check is less reliable now

    # 3. Test invalidation (if applicable based on cache key)
    # The plan notes the key might just be CWD. Let's test if touching .clinerules invalidates.
    # If this assertion fails, it means the cache *doesn't* depend on .clinerules mtime.
    print("Touching .clinerules...")
    clinerules_path = project_root_path / ".clinerules"
    touch(clinerules_path)
    time.sleep(0.1) # Ensure mtime change is noticeable

    print("Calling get_project_root after touching .clinerules...")
    root3 = path_utils.get_project_root()
    assert path_utils.normalize_path(root3) == path_utils.normalize_path(root1) # Compare normalized
    stats3 = get_cache_stats(cache_name)

    # Assert based on expected behavior: The cache key for get_project_root
    # is primarily based on CWD at the time of the first call within the process.
    # Modifying .clinerules should NOT invalidate the cache.
    # Therefore, we expect another cache HIT here.
    assert stats3['hits'] > stats2.get('hits', 0), \
        "Expected another cache hit after touching .clinerules, as mtime should not affect this cache."
    # Verify miss count did NOT increase
    assert stats3.get('misses', 0) == stats2.get('misses', 0), \
        "Expected cache miss count to remain the same after touching .clinerules."

    # Note: If the cache *did* unexpectedly depend on .clinerules mtime,
    # the assertion on line 136 would fail, indicating unexpected behavior.

# Test Case FC-02: path_utils.is_valid_project_path Cache
def test_fc02_is_valid_project_path_cache(test_project, clear_cache_fixture, caplog): # Added fixture arg
    clear_all_caches() # Manual clear at start
    """Verify is_valid_project_path cache hits/misses based on path and root."""
    # Assume cache name is 'is_valid_project_path', adjust if needed
    cache_name = 'is_valid_project_path'
    project_root_path = test_project
    valid_path_rel = "src/module_a.py"
    valid_path_abs = project_root_path / valid_path_rel
    invalid_path_rel = "non_existent/file.txt"

    # Ensure the underlying get_project_root is called at least once
    path_utils.get_project_root()

    # 1. Initial call (valid path)
    print(f"Calling is_valid_project_path for '{valid_path_rel}' (1st time)")
    res1 = path_utils.is_valid_project_path(str(valid_path_abs))
    assert res1 is True
    # Don't assert initial miss count
    stats1 = get_cache_stats(cache_name)

    # 2. Second call (same valid path) -> Cache Hit
    print(f"Calling is_valid_project_path for '{valid_path_rel}' (2nd time)")
    res2 = path_utils.is_valid_project_path(str(valid_path_abs))
    assert res2 is True
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Expect hit on second call
    # assert stats2['hits'] > stats1.get('hits', 0)

    # 3. Third call (different valid path) -> Cache Miss (different path argument)
    # Note: Creating another file just to test a different valid path
    another_valid_path_rel = "docs/readme.md"
    another_valid_path_abs = project_root_path / another_valid_path_rel
    print(f"Calling is_valid_project_path for '{another_valid_path_rel}' (1st time)")
    res3 = path_utils.is_valid_project_path(str(another_valid_path_abs))
    assert res3 is True
    stats3 = get_cache_stats(cache_name)
    # Check miss count increased vs state after call 2
    assert stats3['misses'] > stats2.get('misses', 0)

    # 4. Fourth call (invalid path) -> Cache Miss (different path argument)
    print(f"Calling is_valid_project_path for '{invalid_path_rel}' (1st time)")
    res4 = path_utils.is_valid_project_path(invalid_path_rel)
    assert res4 is False
    stats4 = get_cache_stats(cache_name)
    # Check miss count increased vs state after call 3
    assert stats4['misses'] > stats3.get('misses', 0)

    # 5. Fifth call (same invalid path) -> Cache Hit
    print(f"Calling is_valid_project_path for '{invalid_path_rel}' (2nd time)")
    res5 = path_utils.is_valid_project_path(invalid_path_rel)
    assert res5 is False
    stats5 = get_cache_stats(cache_name)
    # Check hit count increased vs state after call 4
    assert stats5['hits'] > stats4.get('hits', 0)

    # 6. Test potential invalidation via dependency (e.g., if get_project_root cache invalidated)
    # As per FC-01 test, touching .clinerules might not invalidate get_project_root.
    # If get_project_root's cache *did* invalidate, we'd expect a miss here.
    # Since it likely doesn't, we expect another hit for the valid path.
    print("Touching .clinerules (for potential indirect invalidation)...")
    clinerules_path = project_root_path / ".clinerules"
    touch(clinerules_path)
    time.sleep(0.1)

    # Force re-evaluation of get_project_root if it was cached and potentially invalidated
    # path_utils.get_project_root.cache_clear() # Example if clear is possible
    # path_utils.get_project_root()

    print(f"Calling is_valid_project_path for '{valid_path_rel}' (after touch)")
    res6 = path_utils.is_valid_project_path(str(valid_path_abs))
    assert res6 is True
    stats6 = get_cache_stats(cache_name)
    # Assert based on expected behavior: Since get_project_root's cache was likely
    # not invalidated by touching .clinerules (as confirmed in FC-01),
    # and is_valid_project_path depends on the (cached) project root value,
    # this cache should also NOT be invalidated. Expect a cache HIT.
    assert stats6['hits'] > stats5.get('hits', 0), \
        "Expected another cache hit for the same path after touching .clinerules."
    assert stats6.get('misses', 0) == stats5.get('misses', 0), \
        "Expected cache miss count to remain the same after touching .clinerules."

# Test Case FC-03: embedding_manager.calculate_similarity Cache
# Note: This test assumes calculate_similarity can locate the embeddings
# in the test_project fixture's cache/embeddings directory.
# This might require mocking ConfigManager or passing the path explicitly.
def test_fc03_calculate_similarity_cache(test_project, clear_cache_fixture, monkeypatch, caplog): # Added fixture
    clear_all_caches() # Manual clear
    """Verify calculate_similarity cache hits/misses based on keys and .npy mtime."""
    cache_name = 'calculate_similarity' # Adjust if needed
    embedding_dir = test_project / "cache" / "embeddings"
    key1 = '1A1'
    key2 = '1B2'
    key3 = '1C3'
    npy1_path = embedding_dir / f"{key1}.npy"
    npy2_path = embedding_dir / f"{key2}.npy"
    npy3_path = embedding_dir / f"{key3}.npy"

    # Mock necessary functions if calculate_similarity doesn't take path directly
    # Example: monkeypatch.setattr(config_manager.ConfigManager, 'get_embedding_dir', lambda: embedding_dir)
    # Or assume calculate_similarity is modified/mockable for testing

    # --- Cache Key Function ---
    # We will use the *real* cache key function associated with the @cached
    # decorator on embedding_manager.calculate_similarity.
    # This ensures the test verifies the actual key generation logic,
    # including its dependency on .npy file mtimes.
    # (Removing the previous mock for _get_similarity_cache_key)

    # Note: If the real key function requires specific context arguments
    # (e.g., config object), ensure they are available. The test setup
    # might need adjustments if running fails due to missing context.
    # Assume for now that the necessary context (like embedding path from config)
    # is implicitly handled or mocked elsewhere if needed.




    # --- Mocking embedding loading for simplicity ---
    # Let's mock the actual loading to avoid numpy dependency issues in test setup
    # and focus purely on the caching mechanism based on keys and mtime checks.
    mock_embeddings = {
        key1: np.load(npy1_path),
        key2: np.load(npy2_path),
        key3: np.load(npy3_path)
    }
    def mock_load_embedding(key, embedding_path=None): # Adjusted signature
        # The real function's cache key should depend on the *mtime* of the file,
        # even if we mock the loading itself.
        # The cache decorator needs to handle the mtime check.
        print(f"Mock loading embedding for key: {key}")
        # Simulate loading based on the mocked data
        return mock_embeddings.get(key)

    # Need to find where load_embedding is called *within* calculate_similarity
    # or how calculate_similarity gets the embedding data. Assuming it uses
    # a helper like `embedding_manager.load_embedding`.
    try:
        # Attempt to patch a potential helper function
        monkeypatch.setattr(embedding_manager, 'load_embedding', mock_load_embedding, raising=False)
    except AttributeError:
         # If load_embedding isn't directly in embedding_manager, adjust the target path
         print("Warning: Could not directly mock embedding_manager.load_embedding. Cache test relies on mtime checks.")
         # As a fallback, the test will rely *only* on the @cached decorator correctly
         # incorporating the file paths derived from keys into its mtime checks.


    # 1. Initial call (key1, key2)
    print(f"\nCalling calculate_similarity({key1}, {key2}) (1st time)")
    sim1 = embedding_manager.calculate_similarity(key1, key2)
    assert isinstance(sim1, float) # Should return a float score
    # Don't assert initial miss count
    stats1 = get_cache_stats(cache_name)

    # 2. Second call (key1, key2) -> Cache Hit
    print(f"Calling calculate_similarity({key1}, {key2}) (2nd time)")
    sim2 = embedding_manager.calculate_similarity(key1, key2)
    assert sim2 == sim1
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Hit expected
    # assert stats2['hits'] > stats1.get('hits', 0)

    # 3. Third call (key2, key1) -> Cache Hit (order invariant)
    print(f"Calling calculate_similarity({key2}, {key1}) (1st time)")
    sim3 = embedding_manager.calculate_similarity(key2, key1)
    assert sim3 == sim1
    stats3 = get_cache_stats(cache_name)
    assert stats3['hits'] > stats2.get('hits', 0) # Check increase from previous state

    # 4. Touch .npy file for key1
    print(f"Touching {npy1_path}...")
    touch(npy1_path)
    time.sleep(0.1) # Ensure mtime is different

    # 5. Call with key1, key2 again -> Cache Miss (due to mtime change)
    print(f"Calling calculate_similarity({key1}, {key2}) (after touch)")
    sim4 = embedding_manager.calculate_similarity(key1, key2)
    # Similarity might be the same if underlying data didn't change, but cache should miss
    assert isinstance(sim4, float)
    stats4 = get_cache_stats(cache_name)
    assert stats4['misses'] > stats3.get('misses', 0) # Check increase from previous state

    # 6. Call with key1, key2 again -> Cache Hit (new result cached)
    print(f"Calling calculate_similarity({key1}, {key2}) (after miss & re-cache)")
    sim5 = embedding_manager.calculate_similarity(key1, key2)
    assert sim5 == sim4
    stats5 = get_cache_stats(cache_name)
    assert stats5['hits'] > stats4.get('hits', 0) # Check increase from previous state

    # 7. Call with new key pair (key1, key3) -> Cache Miss
    print(f"Calling calculate_similarity({key1}, {key3}) (1st time)")
    sim6 = embedding_manager.calculate_similarity(key1, key3)
    assert isinstance(sim6, float)
    stats6 = get_cache_stats(cache_name)
    assert stats6['misses'] > stats5.get('misses', 0) # Check increase from previous state

    # 8. Call with new key pair (key1, key3) again -> Cache Hit
    print(f"Calling calculate_similarity({key1}, {key3}) (2nd time)")
    sim7 = embedding_manager.calculate_similarity(key1, key3)
    assert sim7 == sim6
    stats7 = get_cache_stats(cache_name)
    assert stats7['hits'] > stats6.get('hits', 0) # Check increase from previous state

# Test Case FC-04: dependency_grid.validate_grid Cache
def test_fc04_validate_grid_cache(clear_cache_fixture): # Added fixture
    clear_all_caches() # Manual clear
    """Verify validate_grid cache hits/misses based on grid hash and keys."""
    cache_name = 'validate_grid' # Adjust if needed

    # 1. Create a slightly more complex valid grid and keys for testing reordering
    keys1_unsorted = ['1B', '1A1', '1A2']
    # Assume hierarchical sort results in ['1A1', '1A2', '1B']
    keys1_sorted = ['1A1', '1A2', '1B']
    grid1 = {
        '1A1': "o<p",
        '1A2': ">ox",
        '1B':  "pxo"
    }

    # 2. Initial call (Grid1, Sorted Keys)
    print("\nCalling validate_grid(G1, K1_sorted) (1st time)")
    # Pass the canonical sorted list first
    res1 = dependency_grid.validate_grid(grid1, keys1_sorted)
    assert res1 is True # Assuming this grid is valid
    stats1 = get_cache_stats(cache_name)
    assert stats1.get('misses', 0) >= 1 # Expect initial miss

    # 3. Second call (Grid1, Sorted Keys again) -> Cache Hit
    print("Calling validate_grid(G1, K1_sorted) (2nd time)")
    res2 = dependency_grid.validate_grid(grid1, keys1_sorted)
    assert res2 == res1
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Hit expected

    # 4. Third call (Grid1, UNSORTED Keys) -> Cache HIT (Key uses sorted version)
    print("Calling validate_grid(G1, K1_unsorted) -> Expect HIT")
    res3 = dependency_grid.validate_grid(grid1, keys1_unsorted)
    assert res3 == res1 # Result should be the same
    stats3 = get_cache_stats(cache_name)
    assert stats3['hits'] > stats2.get('hits', 0), "Expected cache hit when calling with unsorted keys."

    # Renumber subsequent steps
    # 5. Test with invalid diagonal -> Cache Miss & False
    print("Calling validate_grid with invalid diagonal ('p') -> Expect Miss + False")
    # Use the same sorted keys, but modify the grid content
    grid_invalid_diag = { '1A1': "p<p", '1A2': ">px", '1B': "xxp" } # Invalid 'p' on diagonals
    res4 = dependency_grid.validate_grid(grid_invalid_diag, keys1_sorted)
    assert res4 is False, "Validation should fail with incorrect diagonal 'p'."
    stats4 = get_cache_stats(cache_name)
    # Check miss count increased vs state after the UNSORTED call hit (stats3)
    assert stats4['misses'] > stats3.get('misses', 0), "Expected cache miss for invalid grid content."


    # Renumber steps
    # 6. Test with different keys list -> Cache Miss & False/Error
    print("Calling validate_grid with different keys list -> Expect Miss + False/Error")
    keys_different = ['1A1', '1A2', '1C'] # Different set of keys
    try:
         # Use original valid grid1, but with the different keys list
         res5 = dependency_grid.validate_grid(grid1, keys_different)
         assert res5 is False, "Validation should fail with different keys list."
    except Exception as e:
         print(f"Caught expected exception for different keys list: {e}")
         pass # Allow exceptions
    stats5 = get_cache_stats(cache_name)
    # Check miss count increased vs state after the invalid grid miss (stats4)
    assert stats5['misses'] > stats4.get('misses', 0), "Expected cache miss for different keys list."

    # Final state check (optional)
    print(f"Final stats for {cache_name}: {stats5}")

    # Note: This simplified test focuses on the diagonal and key matching.
    # Other aspects like symmetry or row length aren't tested here due to previous ambiguity.
# Test Case FC-05: dependency_grid.get_dependencies_from_grid Cache
def test_fc05_get_dependencies_from_grid_cache(clear_cache_fixture): # Added fixture
    clear_all_caches() # Manual clear
    """Verify get_dependencies_from_grid cache based on grid, keys, and target key."""
    cache_name = 'get_dependencies_from_grid' # Adjust if needed

    # 1. Create initial grid and keys (same as FC-04)
    keys1 = ['1A1', '1B', '1A2'] # Unsorted
    keys1_sorted = ['1A1', '1A2', '1B'] # Hierarchically sorted
    # Create grid strings based on the described dependencies
    # Row 1A1: vs [1A1, 1A2, 1B] -> [o, <, p] (assuming self is 'o')
    # Row 1A2: vs [1A1, 1A2, 1B] -> [>, o, x]
    # Row 1B:  vs [1A1, 1A2, 1B] -> [p, x, o]
    grid1 = {
        keys1_sorted[0]: "o<p",
        keys1_sorted[1]: ">ox",
        keys1_sorted[2]: "pxo",
    }
    target_key1 = keys1_sorted[0] # '1A1'
    target_key2 = keys1_sorted[1] # '1A2'

    # Expected results (adjust based on actual get_dependencies_from_grid logic)
    expected_deps1 = {'incoming': {keys1_sorted[1]}, 'outgoing': set()} # 1A2 depends ON 1A1 ('>' from 1A2's perspective)
    # Let's refine based on grid:
    # For 1A1: incoming = {1A2}, outgoing = {} (only sees '>' from 1A2 row)
    # For 1A2: incoming = {}, outgoing = {1A1, 1B} (sees '<' to 1A1, 'x' to 1B)
    # For 1B: incoming = {1A2}, outgoing = {1A2} (sees 'x' to 1A2)

    # Updated expected results based on format {dep_char: [keys_at_that_char_index]}
    # Using the sorted key list: ['1A1', '1A2', '1B']
    # For target '1A1', row is 'o<p'.
    #   Char '<' is at index 1 (key '1A2').
    #   Char 'p' is at index 2 (key '1B').
    expected_deps1 = {'<': [keys1_sorted[1]], 'p': [keys1_sorted[2]]}
    # For target '1A2', row is '>ox'.
    #   Char '>' is at index 0 (key '1A1').
    #   Char 'x' is at index 2 (key '1B').
    expected_deps2 = {'>': [keys1_sorted[0]], 'x': [keys1_sorted[2]]}
    # 2. Initial call (G1, K1, target_key1)
    print(f"\nCalling get_dependencies_from_grid(G1, {target_key1}, K1) (1st time)")
    deps1 = dependency_grid.get_dependencies_from_grid(grid1, target_key1, keys1)
    assert deps1 == expected_deps1
    # Don't assert initial miss count
    stats1 = get_cache_stats(cache_name)

    # 3. Second call (G1, K1, target_key1) -> Cache Hit
    print(f"Calling get_dependencies_from_grid(G1, {target_key1}, K1) (2nd time)")
    deps2 = dependency_grid.get_dependencies_from_grid(grid1, target_key1, keys1)
    assert deps2 == deps1
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Hit expected
    # assert stats2['hits'] > stats1.get('hits', 0)

    # 4. Third call (G1, unsorted K1, target_key1) -> Cache Hit (uses sorted keys)
    print(f"Calling get_dependencies_from_grid(G1, {target_key1}, K1_unsorted) (1st time)")
    deps3 = dependency_grid.get_dependencies_from_grid(grid1, target_key1, ['1B', '1A2', '1A1'])
    assert deps3 == deps1
    stats3 = get_cache_stats(cache_name)
    assert stats3['hits'] > stats2.get('hits', 0) # Increase vs previous

    # 5. Fourth call (G1, K1, different target_key2) -> Cache Miss (different target)
    print(f"Calling get_dependencies_from_grid(G1, {target_key2}, K1) (1st time)")
    deps4 = dependency_grid.get_dependencies_from_grid(grid1, target_key2, keys1)
    assert deps4 == expected_deps2
    stats4 = get_cache_stats(cache_name)
    assert stats4['misses'] > stats3.get('misses', 0) # Increase vs previous

    # 6. Modify grid (G1 -> G2)
    import copy
    grid2 = copy.deepcopy(grid1)
    # Modify 1A1's dependency on 1A2 (index 1) from '<' to 'x' directly
    grid2[keys1_sorted[0]] = "oxo"

    # 7. Call with modified grid (G2, K1, target_key1) -> Cache Miss (grid hash changed)
    print(f"Calling get_dependencies_from_grid(G2, {target_key1}, K1) (1st time)")
    # Recalculate expected for G2, target_key1 ('1A1') where row is now 'oxo'
    # Char 'x' is at index 1 (key '1A2').
    # Char 'o' at index 2 doesn't map to a key in this format (it's '1B'). Let's assume it should be 'p'
    # Modified grid2['1A1'] = "oxp" instead of "oxo" for clarity
    grid2[keys1_sorted[0]] = "oxp"
    # Expected: 'x' maps to '1A2', 'p' maps to '1B'
    expected_deps_g2_t1 = {'x': [keys1_sorted[1]], 'p': [keys1_sorted[2]]}
    deps5 = dependency_grid.get_dependencies_from_grid(grid2, target_key1, keys1)
    assert deps5 == expected_deps_g2_t1
    stats5 = get_cache_stats(cache_name)
    assert stats5['misses'] > stats4.get('misses', 0) # Increase vs previous

    # 8. Call again with G2, K1, target_key1 -> Cache Hit
    print(f"Calling get_dependencies_from_grid(G2, {target_key1}, K1) (2nd time)")
    deps6 = dependency_grid.get_dependencies_from_grid(grid2, target_key1, keys1)
    assert deps6 == deps5
    stats6 = get_cache_stats(cache_name)
    assert stats6['hits'] >= 1 # Hit expected
    # assert stats6['hits'] > stats5.get('hits', 0) # Check hits increased from previous miss

# Test Case FC-06: tracker_io.read_tracker_file Cache
def test_fc06_read_tracker_file_cache(test_project, clear_cache_fixture): # Added fixture
    clear_all_caches() # Manual clear
    """Verify read_tracker_file cache invalidates on file modification."""
    cache_name = 'read_tracker_file' # Adjust if needed
    tracker_rel_path = "cline_docs/tracker.md"
    tracker_abs_path = test_project / tracker_rel_path

    # Ensure the file exists and has some initial content
    initial_content = "# Initial Tracker\nkey1,key2\np,p\n"
    tracker_abs_path.write_text(initial_content)
    time.sleep(0.1) # Ensure write completes and mtime is set

    # 1. Initial read
    print(f"\nCalling read_tracker_file({tracker_rel_path}) (1st time)")
    # Assuming read_tracker_file returns content and maybe keys/grid
    # Adjust assertion based on actual return type
    result1 = tracker_io.read_tracker_file(str(tracker_abs_path))
    assert result1 is not None # Basic check, refine based on return value
    # Example if it returns a tuple (content_str, keys, grid):
    # assert result1[0] == initial_content

    # Don't assert initial miss count
    stats1 = get_cache_stats(cache_name)

    # 2. Second read -> Cache Hit
    print(f"Calling read_tracker_file({tracker_rel_path}) (2nd time)")
    result2 = tracker_io.read_tracker_file(str(tracker_abs_path))
    assert result2 == result1 # Should return the same cached result
    stats2 = get_cache_stats(cache_name)
    assert stats2['hits'] >= 1 # Hit expected
    # assert stats2['hits'] > stats1.get('hits', 0)

    # 3. Touch the tracker file
    print(f"Touching {tracker_abs_path}...")
    touch(tracker_abs_path)
    time.sleep(0.1) # Ensure mtime change is noticeable

    # 4. Third read -> Cache Miss (mtime changed)
    print(f"Calling read_tracker_file({tracker_rel_path}) (after touch)")
    result3 = tracker_io.read_tracker_file(str(tracker_abs_path))
    # Result should still be the same content, but fetched fresh
    assert result3 == result1
    stats3 = get_cache_stats(cache_name)
    assert stats3.get('misses', 0) > stats2.get('misses', 0), \
        f"Expected cache miss count ({stats3.get('misses', 0)}) to increase after touch (was {stats2.get('misses', 0)})."
    assert stats3['misses'] > stats2.get('misses', 0) # Increase vs previous

    # 5. Modify the tracker file content
    modified_content = "# Modified Tracker\nkey1,key3\np,<\n"
    print(f"Modifying content of {tracker_abs_path}...")
    tracker_abs_path.write_text(modified_content)
    time.sleep(0.1)

    # 6. Fourth read -> Cache Miss (mtime changed again)
    print(f"Calling read_tracker_file({tracker_rel_path}) (after modify)")
    result4 = tracker_io.read_tracker_file(str(tracker_abs_path))
    # Result should now reflect the modified content
    # Adjust assertion based on return value
    assert result4 != result1
    # Example: assert result4[0] == modified_content
    stats4 = get_cache_stats(cache_name)
    assert stats4['misses'] > stats3.get('misses', 0) # Increase vs previous

    # 7. Fifth read -> Cache Hit (new content cached)
    print(f"Calling read_tracker_file({tracker_rel_path}) (after re-cache)")
    result5 = tracker_io.read_tracker_file(str(tracker_abs_path))
    assert result5 == result4
    stats5 = get_cache_stats(cache_name)
    assert stats5['hits'] >= 1 # Hit expected
    # assert stats5['hits'] > stats4.get('hits', 0)

# Test Case FC-07: Config-dependent Caches
def test_fc07_config_dependent_caches(test_project, clear_cache_fixture, monkeypatch): # Added fixture
    clear_all_caches() # Manual clear
    """Verify caches invalidate when .clinerules.config.json changes."""
    config_path = test_project / ".clinerules.config.json"
    # Assume cache names - adjust if necessary
    cache_name_config = 'ConfigManager.get_excluded_dirs'
    cache_name_valid_file = '_is_valid_file' # Associated with embedding_manager?

    # --- Setup ---
    # Ensure a known initial config state
    initial_config_data = {
        "paths": {"doc_dir": "docs", "memory_dir": "cline_docs"},
        "exclusions": {"dirs": [".git", "venv"], "files": ["*.log", "*.tmp"]},
        "thresholds": {"code_similarity": 0.7}
    }
    config_path.write_text(json.dumps(initial_config_data))
    time.sleep(0.1)

    # Instantiate ConfigManager - assumes it reads config on init or methods are cached correctly
    # We might need to force a reload or clear its internal state if it holds config data directly
    # Forcing reload for safety:
    monkeypatch.setattr(config_manager, '_instance', None, raising=False)
    cm = config_manager.ConfigManager()
    initial_exclusions = cm.get_excluded_dirs() # Call once to load/cache

    # Assuming _is_valid_file exists in embedding_manager as per user comment
    # Use a path that would initially be valid according to config
    test_file_path_valid = str(test_project / "src/some_code.py")
    test_file_path_excluded = str(test_project / "data/log.log") # Matches *.log exclusion
    # Call _is_valid_file once to load/cache
    try:
        is_valid1 = embedding_manager._is_valid_file(test_file_path_valid)
        is_valid_excluded1 = embedding_manager._is_valid_file(test_file_path_excluded)
    except AttributeError:
        pytest.skip("Skipping _is_valid_file test: function not found in embedding_manager")
        return # Skip rest of the test if function isn't there

    # --- Initial Calls & Cache Hits ---
    # 1. Call config getter again -> Hit
    print("\nCalling get_excluded_dirs() (2nd time)")
    exclusions1 = cm.get_excluded_dirs()
    assert exclusions1 == initial_exclusions
    # Don't assert initial hit count
    stats_conf1 = get_cache_stats(cache_name_config)

    # 2. Call _is_valid_file again -> Hit
    print(f"Calling _is_valid_file({test_file_path_valid}) (2nd time)")
    is_valid2 = embedding_manager._is_valid_file(test_file_path_valid)
    assert is_valid2 == is_valid1
    # Don't assert initial hit count
    stats_valid1 = get_cache_stats(cache_name_valid_file)

    # --- Modify Config File ---
    print(f"Modifying {config_path}...")
    modified_config_data = {
        "paths": {"doc_dir": "docs", "memory_dir": "cline_docs"},
        "exclusions": {"dirs": [".git", "venv", "build"], "files": ["*.log"]}, # Removed *.tmp, added build/
        "thresholds": {"code_similarity": 0.8}
    }
    config_path.write_text(json.dumps(modified_config_data))
    time.sleep(0.1) # Ensure mtime change

    # --- Calls After Config Change -> Cache Misses ---
    # Clear ConfigManager singleton instance to force reload on next call
    monkeypatch.setattr(config_manager, '_instance', None, raising=False)
    cm = config_manager.ConfigManager() # Re-instantiate

    # 3. Call config getter again -> Miss
    print("Calling get_excluded_dirs() (after modify)")
    exclusions2 = cm.get_excluded_dirs()
    assert exclusions2 != initial_exclusions
    assert "build" in exclusions2 # Check new value loaded
    stats_conf2 = get_cache_stats(cache_name_config)
    assert stats_conf2['misses'] > stats_conf1.get('misses', 0) # Increase vs previous

    # 4. Call _is_valid_file again -> Miss
    # The validity might change based on new exclusions, but cache must miss
    print(f"Calling _is_valid_file({test_file_path_valid}) (after modify)")
    is_valid3 = embedding_manager._is_valid_file(test_file_path_valid)
    # Validity of this specific file probably didn't change, but check cache stats
    stats_valid2 = get_cache_stats(cache_name_valid_file)
    assert stats_valid2['misses'] > stats_valid1.get('misses', 0) # Increase vs previous

    # Test with a path whose validity *did* change (tmp file no longer excluded)
    tmp_file_path = str(test_project / "output.tmp")
    (test_project / "output.tmp").touch() # Create the file
    print(f"Calling _is_valid_file({tmp_file_path}) (after modify)")
    is_valid_tmp = embedding_manager._is_valid_file(tmp_file_path)
    assert is_valid_tmp is True # Should now be valid as *.tmp is not excluded
    stats_valid3 = get_cache_stats(cache_name_valid_file)
    assert stats_valid3['misses'] > stats_valid2.get('misses', 0) # Increase vs previous


    # --- Calls After Misses -> Cache Hits ---
    # 5. Call config getter again -> Hit
    print("Calling get_excluded_dirs() (after re-cache)")
    exclusions3 = cm.get_excluded_dirs()
    assert exclusions3 == exclusions2
    stats_conf3 = get_cache_stats(cache_name_config)
    assert stats_conf3['hits'] >= 1 # Expect at least one hit after miss
    # assert stats_conf3['hits'] > stats_conf2.get('hits', 0)

    # 6. Call _is_valid_file again -> Hit
    print(f"Calling _is_valid_file({test_file_path_valid}) (after re-cache)")
    is_valid4 = embedding_manager._is_valid_file(test_file_path_valid)
    assert is_valid4 == is_valid3
    stats_valid4 = get_cache_stats(cache_name_valid_file)
    assert stats_valid4['hits'] >= 1 # Expect at least one hit after miss
    # assert stats_valid4['hits'] > stats_valid3.get('hits', 0)


# --- Integration Tests (IS-01 to IS-04) ---

# Test Case IS-01: Source File Modification
def test_is01_source_file_modification(test_project): # Removed monkeypatch fixture
    """Verify analyze_project updates trackers correctly after source file change."""
    project_root = test_project
    tracker_path = project_root / "cline_docs" / "module_relationship_tracker.md" # Adjust if needed
    # Using main tracker for simplicity, could also check mini-trackers
    # Ensure the test_project fixture is set up correctly (CWD change)

    # No need to mock clear_all_caches for this test.
    # We rely on analyze_project's internal cache invalidation based on mtime.











    # 1. Initial analysis run
    print("\nRunning analyze_project (initial run)...")
    try:
        # Use force_analysis to ensure trackers are generated based on current state
        results1 = analyze_project(force_analysis=True)
    except Exception as e:
        pytest.fail(f"Initial analyze_project failed: {e}")

    # Check if tracker file exists and read its initial state
    if not tracker_path.exists():
        # If analyze_project doesn't create it by default, we might need to manually create an empty one first
        # For now, assume it should be created or fail
        pytest.fail(f"Initial run did not create tracker file: {tracker_path}")

    tracker_data1 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data1 or 'grid' not in tracker_data1 or 'keys' not in tracker_data1:
         pytest.fail("Could not read valid initial tracker data.")
    grid1 = tracker_data1['grid']
    keys1 = tracker_data1['keys']
    
    # Find keys corresponding to the test files using normalize_path for consistency
    module_a_norm_path = path_utils.normalize_path(str(project_root / "src" / "module_a.py"))
    module_b_norm_path = path_utils.normalize_path(str(project_root / "src" / "module_b.py"))

    key_a = next((k for k, p in keys1.items() if path_utils.normalize_path(p) == module_a_norm_path), None)
    key_b = next((k for k, p in keys1.items() if path_utils.normalize_path(p) == module_b_norm_path), None)
    
    if not key_a or not key_b:
        print(f"Keys found: {keys1}")
        print(f"Looking for: {module_a_norm_path} and {module_b_norm_path}")
        pytest.fail(f"Could not find keys for module_a.py or module_b.py in initial tracker.")

    # Assuming initially no dependency between a and b
    # Note: The actual dependency might be 'p' or 'n' depending on initial analysis
    sorted_keys1 = sorted(keys1.keys())
    initial_dep_char = dependency_grid.get_char_at_key(grid1, key_a, key_b, sorted_keys1)
    print(f"Initial dependency char {key_a} -> {key_b}: '{initial_dep_char}'")


    # 2. Modify a source file to add a dependency
    module_a_path_obj = project_root / "src" / "module_a.py"
    print(f"Modifying {module_a_path_obj} to import module_b...")
    module_a_path_obj.write_text("import os\nimport module_b # Add import\nprint(module_b)")
    time.sleep(0.1) # Ensure mtime change

    # 3. Run analysis again
    print("Running analyze_project (after modification)...")
    # Rely on analyze_project's internal cache handling (mtime checks in analyze_file)
    try:
        results2 = analyze_project() # Don't force, let caching work (except for modified file)
    except Exception as e:
        pytest.fail(f"Second analyze_project run failed: {e}")

    # 4. Compare tracker grid
    if not tracker_path.exists():
         pytest.fail(f"Tracker file missing after second run: {tracker_path}")

    tracker_data2 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data2 or 'grid' not in tracker_data2 or 'keys' not in tracker_data2:
         pytest.fail("Could not read valid tracker data after modification.")
    grid2 = tracker_data2['grid']
    keys2 = tracker_data2['keys'] # Keys might change if analysis adds/removes files

    # Re-find keys in case they changed (unlikely here but good practice)
    key_a_new = next((k for k, p in keys2.items() if path_utils.normalize_path(p) == module_a_norm_path), None)
    key_b_new = next((k for k, p in keys2.items() if path_utils.normalize_path(p) == module_b_norm_path), None)
    
    if not key_a_new or not key_b_new:
         pytest.fail(f"Could not find keys in second tracker. Keys: {keys2}")
    assert key_a_new == key_a # Keys should ideally be stable
    assert key_b_new == key_b

    # Verify the dependency character changed
    sorted_keys2 = sorted(keys2.keys())
    final_dep_char = dependency_grid.get_char_at_key(grid2, key_a, key_b, sorted_keys2)
    print(f"Final dependency char {key_a} -> {key_b}: '{final_dep_char}'")
    # Expect '>' (module_a depends on module_b) or 'x' (if reciprocal detected)
    # Allow 's' or 'S' if semantic analysis picks it up instead of static import
    assert final_dep_char in ('>', 'x', 's', 'S'), f"Expected dependency char '>', 'x', 's', or 'S', but got '{final_dep_char}'"
    assert final_dep_char != initial_dep_char, "Dependency character did not change after modification"

# Test Case IS-02: Config File Modification (Exclusion)
def test_is02_config_file_exclusion(test_project): # Removed monkeypatch fixture
    """Verify analyze_project removes keys for newly excluded files."""
    # No need to mock clear_all_caches for this test.
    # We rely on analyze_project picking up config changes correctly,
    # implicitly using invalidated config-dependent caches.





    project_root = test_project
    tracker_path = project_root / "cline_docs" / "module_relationship_tracker.md"
    config_path = project_root / ".clinerules.config.json"
    file_to_exclude_rel = "src/module_b.py"
    file_to_exclude_abs_norm = path_utils.normalize_path(str(project_root / file_to_exclude_rel))

    # 1. Initial analysis run
    print("\nRunning analyze_project (initial run for IS-02)...")
    try:
        results1 = analyze_project(force_analysis=True)
    except Exception as e:
        pytest.fail(f"Initial analyze_project failed: {e}")

    if not tracker_path.exists():
        pytest.fail(f"Initial run did not create tracker file: {tracker_path}")
    tracker_data1 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data1 or 'keys' not in tracker_data1:
         pytest.fail("Could not read valid initial tracker data.")
    keys1 = tracker_data1['keys']
    key_b_initial = next((k for k, p in keys1.items() if path_utils.normalize_path(p) == file_to_exclude_abs_norm), None)
    assert key_b_initial is not None, f"Key for {file_to_exclude_rel} not found in initial tracker."
    print(f"Initial tracker contains key '{key_b_initial}' for {file_to_exclude_rel}.")

    # 2. Modify config file to exclude module_b.py
    print(f"Modifying {config_path} to exclude {file_to_exclude_rel}...")
    try:
        with open(config_path, 'r') as f:
            config_data = json.load(f)
        
        # Ensure exclusions structure exists
        if 'exclusions' not in config_data: config_data['exclusions'] = {}
        if 'files' not in config_data['exclusions']: config_data['exclusions']['files'] = []
        
        # Add the relative path (or pattern) to exclusions
        # Using the specific relative path here
        config_data['exclusions']['files'].append(file_to_exclude_rel)
        
        config_path.write_text(json.dumps(config_data, indent=4))
        time.sleep(0.1) # Ensure mtime change
    except Exception as e:
        pytest.fail(f"Failed to modify config file: {e}")
        
    # Clear ConfigManager singleton instance to force reload on next call
    monkeypatch.setattr(config_manager, '_instance', None, raising=False)

    # 3. Run analysis again
    print("Running analyze_project (after config exclusion)...")
    try:
        # Rely on config mtime check in ConfigManager and downstream cache invalidation
        results2 = analyze_project()
    except Exception as e:
        pytest.fail(f"Second analyze_project run failed: {e}")

    # 4. Verify key removal from tracker
    if not tracker_path.exists():
         pytest.fail(f"Tracker file missing after second run: {tracker_path}")
    tracker_data2 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data2 or 'keys' not in tracker_data2:
         pytest.fail("Could not read valid tracker data after modification.")
    keys2 = tracker_data2['keys']

    key_b_final = next((k for k, p in keys2.items() if path_utils.normalize_path(p) == file_to_exclude_abs_norm), None)
    assert key_b_final is None, f"Key for excluded file {file_to_exclude_rel} was found in final tracker (key: {key_b_final})."
    print(f"Verified: Key for excluded file {file_to_exclude_rel} correctly removed from tracker.")

# Test Case IS-03: .clinerules Modification (Roots)
def test_is03_clinerules_modification_roots(test_project): # Removed monkeypatch fixture
    """Verify analyze_project picks up new roots from modified .clinerules."""
    # No need to mock clear_all_caches for this test.
    # We rely on analyze_project picking up .clinerules changes correctly,
    # implicitly using invalidated caches (e.g., for root dirs).





    project_root = test_project
    tracker_path = project_root / "cline_docs" / "module_relationship_tracker.md"
    clinerules_path = project_root / ".clinerules"
    new_root_rel = "lib"
    new_file_rel = f"{new_root_rel}/helper.py"
    new_file_abs_norm = path_utils.normalize_path(str(project_root / new_file_rel))

    # --- Setup: Initial .clinerules content ---
    # We need to ensure .clinerules has the expected sections first
    # Using ConfigManager might be better, but simple string manipulation for test
    initial_clinerules_content = """
[COUNT]
n + 1 = (x)
*This must be displayed at the top of every response.*

[LAST_ACTION_STATE]
last_action: "Test Init"
current_phase: "Execution"
next_action: "Complete Test"
next_phase: "Execution"

---

[CODE_ROOT_DIRECTORIES]
- src

[DOC_DIRECTORIES]
- docs

[LEARNING_JOURNAL]
- Test entry.

[Character_Definitions]
```
- <: Row depends on column.
```
---
**IMPORTANT**
1. Understand the Objective
"""
    clinerules_path.write_text(initial_clinerules_content)
    time.sleep(0.1)

    # 1. Initial analysis run (with only 'src' as code root)
    print("\nRunning analyze_project (initial run for IS-03)...")
    try:
        results1 = analyze_project(force_analysis=True)
    except Exception as e:
        pytest.fail(f"Initial analyze_project failed: {e}")

    if not tracker_path.exists():
        pytest.fail(f"Initial run did not create tracker file: {tracker_path}")
    tracker_data1 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data1 or 'keys' not in tracker_data1:
         pytest.fail("Could not read valid initial tracker data.")
    keys1 = tracker_data1['keys']
    key_helper_initial = next((k for k, p in keys1.items() if path_utils.normalize_path(p) == new_file_abs_norm), None)
    assert key_helper_initial is None, f"Key for {new_file_rel} found in initial tracker, but 'lib' was not a root."

    # 2. Modify .clinerules to add 'lib' as a code root
    print(f"Modifying {clinerules_path} to add '{new_root_rel}' to CODE_ROOT_DIRECTORIES...")
    try:
        current_content = clinerules_path.read_text()
        # Simple string replacement - might be fragile
        modified_content = current_content.replace(
            "[CODE_ROOT_DIRECTORIES]\n- src",
            f"[CODE_ROOT_DIRECTORIES]\n- src\n- {new_root_rel}"
        )
        if modified_content == current_content: # Fallback if pattern didn't match
             pytest.fail("Could not find '[CODE_ROOT_DIRECTORIES]\n- src' to modify in .clinerules")
        
        clinerules_path.write_text(modified_content)
        time.sleep(0.1) # Ensure mtime change
    except Exception as e:
        pytest.fail(f"Failed to modify .clinerules file: {e}")

    # Caches depending on .clinerules mtime should invalidate.
    # This includes ConfigManager reading roots, and potentially path_utils.get_project_root

    # 3. Run analysis again
    print("Running analyze_project (after .clinerules modification)...")
    try:
        # ConfigManager should reload due to mtime change check within it or its cache decorator
        # analyze_project uses ConfigManager to get roots
        results2 = analyze_project()
    except Exception as e:
        pytest.fail(f"Second analyze_project run failed: {e}")

    # 4. Verify new key exists in tracker
    if not tracker_path.exists():
         pytest.fail(f"Tracker file missing after second run: {tracker_path}")
    tracker_data2 = tracker_io.read_tracker_file(str(tracker_path))
    if not tracker_data2 or 'keys' not in tracker_data2:
         pytest.fail("Could not read valid tracker data after modification.")
    keys2 = tracker_data2['keys']

    key_helper_final = next((k for k, p in keys2.items() if path_utils.normalize_path(p) == new_file_abs_norm), None)
    assert key_helper_final is not None, f"Key for newly added root file {new_file_rel} was NOT found in final tracker."
    print(f"Verified: Key '{key_helper_final}' for new root file {new_file_rel} correctly added to tracker.")

# Test Case IS-04: Embedding Regeneration
def test_is04_embedding_regeneration(test_project): # Removed monkeypatch fixture
    """Verify --force-embeddings works and similarity cache updates."""
    # No mocking needed for clear_all_caches or calculate_similarity.
    # We want to test the *real* interaction after forced embedding regeneration.

    project_root = test_project
    tracker_path = project_root / "cline_docs" / "module_relationship_tracker.md"
    module_a_rel = "src/module_a.py"
    module_b_rel = "src/module_b.py"
    module_a_abs_norm = path_utils.normalize_path(str(project_root / module_a_rel))
    module_b_abs_norm = path_utils.normalize_path(str(project_root / module_b_rel))
    
    # Define cache name for similarity if needed for stats (optional here)
    cache_name_sim = 'calculate_similarity'

    # Ensure embedding dir exists for calculate_similarity to work
    embedding_dir = project_root / "cache" / "embeddings"
    embedding_dir.mkdir(parents=True, exist_ok=True)
    # Mock ConfigManager to point to the test embedding dir if necessary
    # This might depend on how calculate_similarity finds the dir
    # Example (if ConfigManager is used):
    # from unittest.mock import patch
    # with patch.object(config_manager.ConfigManager, 'get_embedding_dir', return_value=embedding_dir):
    #    # ... rest of the test ...
    # Assuming for now it finds it relative to project root or via config already set up






























    # 1. Initial analysis run to establish baseline keys and embeddings
    print("\nRunning analyze_project (initial run for IS-04)...")
    try:
        results1 = analyze_project(force_analysis=True) # Ensure embeddings generated
    except Exception as e:
        pytest.fail(f"Initial analyze_project failed: {e}")
        
    if not tracker_path.exists(): pytest.fail("Tracker file not created.")
    tracker_data1 = tracker_io.read_tracker_file(str(tracker_path))
    keys_map = tracker_data1.get('keys', {}) # Get key map for mock
    key_a = next((k for k, p in keys_map.items() if path_utils.normalize_path(p) == module_a_abs_norm), None)
    key_b = next((k for k, p in keys_map.items() if path_utils.normalize_path(p) == module_b_abs_norm), None)
    if not key_a or not key_b: pytest.fail("Could not find keys for module_a or module_b.")

    # Call REAL similarity function to get a baseline
    print("Calling initial REAL similarity calculation...")
    initial_sim = embedding_manager.calculate_similarity(key_a, key_b)
    print(f" -> Initial Similarity: {initial_sim}")

    # 2. Modify a source file significantly
    module_a_path_obj = project_root / module_a_rel
    print(f"Modifying {module_a_path_obj} significantly...")
    new_content_a = "class NewClass:\n def method(self):\n  pass\n# Completely different content hash"
    module_a_path_obj.write_text(new_content_a)
    # content_a_new_hash = hash(new_content_a) # Not needed without mock
    time.sleep(0.1)

    # 3. Run analysis with force_embeddings=True
    print("Running analyze_project --force-embeddings...")
    try:
        results2 = analyze_project(force_embeddings=True)
    except Exception as e:
        pytest.fail(f"analyze_project --force-embeddings run failed: {e}")

    # 4. Call similarity manually *after* the forced embedding run
    print("Calling REAL similarity calculation *after* force-embeddings run...")
    # Ensure the keys are still valid (they shouldn't change in this scenario)
    tracker_data2 = tracker_io.read_tracker_file(str(tracker_path))
    keys_map_after = tracker_data2.get('keys', {})
    key_a_after = next((k for k, p in keys_map_after.items() if path_utils.normalize_path(p) == module_a_abs_norm), None)
    key_b_after = next((k for k, p in keys_map_after.items() if path_utils.normalize_path(p) == module_b_abs_norm), None)
    assert key_a_after == key_a, "Key for module_a changed unexpectedly."
    assert key_b_after == key_b, "Key for module_b changed unexpectedly."

    final_sim = embedding_manager.calculate_similarity(key_a, key_b)
    print(f" -> Final Similarity: {final_sim}")

    # Assertions:
    # Check that the similarity score changed (because content hash changed)
    # Note: This assumes the embedding function is sensitive enough to the change.
    # A small tolerance might be needed if embeddings are very similar.
    assert final_sim != initial_sim, f"Similarity score ({final_sim}) did not change after modifying file and forcing embeddings (initial: {initial_sim}). Cache might not have updated."

    print("Verified: Embedding regeneration appears to have triggered use of new data for similarity.")

    print("Running analyze_project --force-embeddings...")
    try:
        results2 = analyze_project(force_embeddings=True)
    except Exception as e:
        pytest.fail(f"analyze_project --force-embeddings run failed: {e}")

    # 4. Call similarity manually *after* the forced embedding run
    print("Calling similarity calculation *after* force-embeddings run...")
    calc_sim_calls.clear() # Clear previous calls before manual check
    final_sim = mocked_calculate_similarity(key_a, key_b)
    
    # Assertions:
    # a) Check that the mock was called (meaning the cache didn't prevent it)
    assert len(calc_sim_calls) > 0, "calculate_similarity mock was not called after force-embeddings run."
    
    # b) Check that the mock used the *new* content hash for the modified file
    last_call = calc_sim_calls[-1]
    assert last_call['content1_hash'] == content_a_new_hash or last_call['content2_hash'] == content_a_new_hash, \
        f"Similarity calculation did not use the new content hash for {module_a_rel}. Hashes used: {last_call['content1_hash']}, {last_call['content2_hash']}. Expected: {content_a_new_hash}"

    # c) Check that the similarity score changed (because content hash changed)
    assert final_sim != initial_sim, f"Similarity score ({final_sim}) did not change after modifying file and forcing embeddings (initial: {initial_sim})."

    print("Verified: Embedding regeneration appears to have triggered use of new data for similarity.")

# --- Manual/Tooling Tests (MT-01 to MT-03) ---

# Test Case MT-01: Cache Clearing
def test_mt01_cache_clearing(test_project):
    """Verify the clear-caches command handler clears caches effectively."""
    # Use cacheable functions that analyze_project likely calls
    cache_name_read = 'read_tracker_file'
    cache_name_analyze = 'analyze_file'
    cache_name_config_get = 'ConfigManager.get_excluded_dirs' # Example config cache
    
    tracker_path = test_project / "cline_docs" / "module_relationship_tracker.md"
    file_to_analyze = test_project / "src" / "module_a.py"
    config_path = test_project / ".clinerules.config.json" # Needed for config cache

    # 1. Run analyze_project to populate caches
    print("\nRunning analyze_project (to populate caches for MT-01)...")
    try:
        results1 = analyze_project(force_analysis=True) # Force ensures files are analyzed
    except Exception as e:
        pytest.fail(f"analyze_project failed during cache population: {e}")

    # Ensure tracker exists so read_tracker_file can be called meaningfully
    if not tracker_path.exists():
          # If analyze_project doesn't guarantee tracker creation, manually create one
          tracker_path.parent.mkdir(parents=True, exist_ok=True)
          tracker_path.write_text("# Placeholder Tracker\nkeyA,keyB\np,p\n")
          print("Warning: Tracker file not found, created placeholder for test.")
          
    # 2. Call cached functions once to ensure they are populated if not called by analyze_project
    #    and then call again to verify a cache hit occurred before clearing.
    print("Calling cached functions to confirm population and get initial hit...")
    _ = tracker_io.read_tracker_file(str(tracker_path)) # Populate
    _ = dependency_analyzer.analyze_file(str(file_to_analyze)) # Populate
    _ = config_manager.ConfigManager().get_excluded_dirs() # Populate

    _ = tracker_io.read_tracker_file(str(tracker_path)) # Hit?
    stats_read1 = get_cache_stats(cache_name_read)
    assert stats_read1.get('hits', 0) >= 1, f"Cache '{cache_name_read}' should have hits before clear."

    _ = dependency_analyzer.analyze_file(str(file_to_analyze)) # Hit?
    stats_analyze1 = get_cache_stats(cache_name_analyze)
    assert stats_analyze1.get('hits', 0) >= 1, f"Cache '{cache_name_analyze}' should have hits before clear."
    
    _ = config_manager.ConfigManager().get_excluded_dirs() # Hit?
    stats_config1 = get_cache_stats(cache_name_config_get)
    assert stats_config1.get('hits', 0) >= 1, f"Cache '{cache_name_config_get}' should have hits before clear."

    # Record miss counts *before* clearing but *after* initial population/hits
    initial_read_misses = stats_read1.get('misses', 0)
    initial_analyze_misses = stats_analyze1.get('misses', 0)
    initial_config_misses = stats_config1.get('misses', 0)

    # 3. Call the clear-caches handler
    print("Calling handle_clear_caches...")
    # The handler takes argparse Namespace object, but doesn't use it. Pass None.
    exit_code = handle_clear_caches(None)
    assert exit_code == 0, "handle_clear_caches returned non-zero exit code."

    # 4. Call cached functions again -> Expect Cache Misses
    print("Calling cached functions after clearing...")
    
    # a) read_tracker_file
    _ = tracker_io.read_tracker_file(str(tracker_path)) # Should miss
    stats_read2 = get_cache_stats(cache_name_read)
    print(f"Cache stats for {cache_name_read} after clear: {stats_read2}")
    assert stats_read2.get('misses', 0) > initial_read_misses, \
        f"Cache '{cache_name_read}' miss count ({stats_read2.get('misses',0)}) should increase after clear (was {initial_read_misses})."

    # b) analyze_file
    _ = dependency_analyzer.analyze_file(str(file_to_analyze)) # Should miss
    stats_analyze2 = get_cache_stats(cache_name_analyze)
    print(f"Cache stats for {cache_name_analyze} after clear: {stats_analyze2}")
    assert stats_analyze2.get('misses', 0) > initial_analyze_misses, \
        f"Cache '{cache_name_analyze}' miss count ({stats_analyze2.get('misses',0)}) should increase after clear (was {initial_analyze_misses})."

    # c) ConfigManager getter
    _ = config_manager.ConfigManager().get_excluded_dirs() # Should miss
    stats_config2 = get_cache_stats(cache_name_config_get)
    print(f"Cache stats for {cache_name_config_get} after clear: {stats_config2}")
    assert stats_config2.get('misses', 0) > initial_config_misses, \
        f"Cache '{cache_name_config_get}' miss count ({stats_config2.get('misses',0)}) should increase after clear (was {initial_config_misses})."

    print("Verified: Cache clearing appears functional.")

# Test Case MT-02: Cache Statistics
def test_mt02_cache_statistics(test_project, clear_cache_fixture): # Added fixture
    """Verify get_cache_stats returns accurate hit/miss counts."""
    clear_all_caches() # Manual clear at start
    # Use a simple cacheable function like read_tracker_file
    cache_name = 'read_tracker_file'
    tracker_path1 = test_project / "cline_docs" / "tracker.md" # Use the one from fixture setup
    tracker_path2 = test_project / "docs" / "readme.md" # Use another file for variety

    # Ensure files exist for the test
    tracker_path1.parent.mkdir(parents=True, exist_ok=True)
    tracker_path1.write_text("# Tracker 1\nA,B\np,p")
    tracker_path2.parent.mkdir(parents=True, exist_ok=True)
    tracker_path2.write_text("# Readme File")
    time.sleep(0.1)

    print(f"\nTesting cache statistics for '{cache_name}'...")

    # Initial state should be 0 hits, 0 misses *after* manual clear
    stats_initial = get_cache_stats(cache_name)
    assert stats_initial.get('hits', 0) == 0, f"Initial hits should be 0 after clear, got {stats_initial}"
    assert stats_initial.get('misses', 0) == 0, f"Initial misses should be 0 after clear, got {stats_initial}"

    # Call 1 (tracker1) -> Miss 1
    print("Call 1 (tracker1)...")
    _ = tracker_io.read_tracker_file(str(tracker_path1))
    stats1 = get_cache_stats(cache_name)
    assert stats1.get('hits', 0) == 0, f"Hits should be 0 after 1st call, got {stats1}"
    assert stats1.get('misses', 0) == 1, f"Misses should be 1 after 1st call, got {stats1}"

    # Call 2 (tracker1 again) -> Hit 1
    print("Call 2 (tracker1)...")
    _ = tracker_io.read_tracker_file(str(tracker_path1))
    stats2 = get_cache_stats(cache_name)
    assert stats2.get('hits', 0) == 1, "Hits should be 1 after 2nd call (same args)."
    assert stats2.get('misses', 0) == 1, "Misses should still be 1."

    # Call 3 (tracker2) -> Miss 2
    print("Call 3 (tracker2)...")
    _ = tracker_io.read_tracker_file(str(tracker_path2))
    stats3 = get_cache_stats(cache_name)
    assert stats3.get('hits', 0) == 1, "Hits should still be 1."
    assert stats3.get('misses', 0) == 2, "Misses should be 2 after 3rd call (diff args)."

    # Call 4 (tracker1 again) -> Hit 2
    print("Call 4 (tracker1)...")
    _ = tracker_io.read_tracker_file(str(tracker_path1))
    stats4 = get_cache_stats(cache_name)
    assert stats4.get('hits', 0) == 2, "Hits should be 2 after 4th call."
    assert stats4.get('misses', 0) == 2, "Misses should still be 2."
    
    # Call 5 (tracker2 again) -> Hit 3
    print("Call 5 (tracker2)...")
    _ = tracker_io.read_tracker_file(str(tracker_path2))
    stats5 = get_cache_stats(cache_name)
    assert stats5.get('hits', 0) == 3, "Hits should be 3 after 5th call."
    assert stats5.get('misses', 0) == 2, "Misses should still be 2."

    # Call 6 (touch tracker1, call tracker1) -> Miss 3
    print("Call 6 (touch tracker1, call tracker1)...")
    touch(tracker_path1)
    time.sleep(0.1)
    _ = tracker_io.read_tracker_file(str(tracker_path1))
    stats6 = get_cache_stats(cache_name)
    assert stats6.get('hits', 0) == 3, "Hits should still be 3."
    assert stats6.get('misses', 0) == 3, "Misses should be 3 after invalidation."

    # Call 7 (tracker1 again) -> Hit 4
    print("Call 7 (tracker1)...")
    _ = tracker_io.read_tracker_file(str(tracker_path1))
    stats7 = get_cache_stats(cache_name)
    assert stats7.get('hits', 0) == 4, "Hits should be 4 after re-cache."
    assert stats7.get('misses', 0) == 3, "Misses should still be 3."

    print(f"Final Stats for '{cache_name}': {stats7}")
    print("Verified: Cache statistics appear accurate.")

# Test Case MT-03: DEBUG Logging
def test_mt03_debug_logging(test_project, clear_cache_fixture, caplog): # Added fixture
    """Verify cache hit/miss/invalidation messages appear in DEBUG logs."""
    clear_all_caches() # Manual clear at start
    import logging
    # Use a simple cacheable function again
    cache_name = 'read_tracker_file'
    tracker_path = test_project / "cline_docs" / "tracker_log_test.md" # Use a unique file

    # Ensure file exists
    tracker_path.parent.mkdir(parents=True, exist_ok=True)
    tracker_path.write_text("# Log Test Tracker\nLogKey1,LogKey2\np,p")
    time.sleep(0.1)

    # Set logging level to DEBUG to capture cache messages
    caplog.set_level(logging.DEBUG)
    # Optionally filter for the specific cache manager logger if known
    # logger_name = 'cline_utils.dependency_system.utils.cache_manager'
    # caplog.set_level(logging.DEBUG, logger=logger_name)

    print(f"\nTesting DEBUG logging for cache '{cache_name}'...")

    # 1. Initial call -> Expect Cache Miss log
    print("Call 1 (expect miss log)...")
    _ = tracker_io.read_tracker_file(str(tracker_path))
    # Check for specific log message if available, otherwise generic check
    # Example: assert f"Cache miss for cache '{cache_name}'" in caplog.text
    assert "miss" in caplog.text.lower(), "Expected 'miss' message in log for initial call."
    print(" -> Miss log found (or expected pattern).")
    caplog.clear() # Clear logs for next step

    # 2. Second call -> Expect Cache Hit log
    print("Call 2 (expect hit log)...")
    _ = tracker_io.read_tracker_file(str(tracker_path))
    # assert f"Cache hit for cache '{cache_name}'" in caplog.text
    assert "hit" in caplog.text.lower(), "Expected 'hit' message in log for second call."
    print(" -> Hit log found.")
    caplog.clear()

    # 3. Touch file -> Expect Invalidation log (if invalidation logs) + Miss log on next call
    print("Touch file (expect invalidation log if implemented)...")
    touch(tracker_path)
    time.sleep(0.1)
    # Note: Invalidation might happen implicitly on the *next* call's check,
    #       or explicitly if there's a separate invalidation mechanism.
    #       The log message might vary. Let's check the logs *after* the next call.

    print("Call 3 (expect miss log after touch)...")
    _ = tracker_io.read_tracker_file(str(tracker_path))
    log_text = caplog.text
    # Check for invalidation message *first* if expected
    # assert "Invalidating cache entry" in log_text # Adjust expected message
    # Then check for miss message again
    assert "miss" in log_text.lower(), "Expected 'miss' message after file touch."
    # If invalidation logs separately:
    # invalidation_logged = "invalidating" in log_text.lower() # Check for specific message
    # print(f" -> Invalidation logged: {invalidation_logged}")
    print(" -> Miss log found after invalidation.")

    print("Verified: DEBUG log messages appear as expected (basic check).")
